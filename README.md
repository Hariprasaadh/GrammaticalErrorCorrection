# Grammatical Error Correction for Low-Resource Indian Languages

This project presents a comprehensive deep learning solution for **Grammatical Error Correction (GEC)** in low-resource Indian languages. Leveraging the power of **Google's mT5 (Multilingual Text-to-Text Transfer Transformer)** architecture, we address the critical challenge of correcting grammatical errors in Indic scripts where annotated data is scarce.

Grammatical error correction is a fundamental task in Natural Language Processing (NLP) that aims to automatically detect and correct grammatical mistakes in written text. While significant progress has been made for high-resource languages like English, Indian languages present unique challenges due to:

- **Limited Training Data:** Most Indian languages have fewer than 100 annotated sentence pairs for GEC
- **Complex Morphology:** Rich inflectional systems and agglutinative word formation
- **Script Diversity:** Multiple writing systems (Devanagari, Tamil, Telugu, Bengali, Malayalam)
- **Code-Mixing:** Frequent mixing of English and native scripts in real-world text

Our solution employs **transfer learning** from mT5-small, a model pre-trained on 101 languages including all major Indian languages, to achieve strong performance even with minimal training data. Through careful optimization of training strategies, data handling, and hyperparameter tuning, we demonstrate that effective GEC systems can be built for low-resource scenarios.


## ğŸ“‹ Task Description

This project focuses on **Grammatical Error Correction (GEC)** for Indian languages in a **low-resource setting**. The task involves correcting grammatical errors in sentences written in:

- ğŸ‡§ğŸ‡© **Bangla**
- ğŸ‡®ğŸ‡³ **Hindi**
- ğŸ‡®ğŸ‡³ **Malayalam**
- ğŸ‡®ğŸ‡³ **Tamil**
- ğŸ‡®ğŸ‡³ **Telugu**

### Performance Metric
**GLEU Score** (Generalized Language Evaluation Understanding) is used as the primary evaluation metric.

## ğŸ—ï¸ Project Structure

```
GrammaticalErrorCorrection/
â”œâ”€â”€ Dataset/
â”‚   â”œâ”€â”€ bangala/
â”‚   â”‚   â”œâ”€â”€ train.csv
â”‚   â”‚   â””â”€â”€ dev.csv
â”‚   â”œâ”€â”€ hindi/
â”‚   â”‚   â”œâ”€â”€ train.csv
â”‚   â”‚   â””â”€â”€ dev.csv
â”‚   â”œâ”€â”€ malayalam/
â”‚   â”‚   â”œâ”€â”€ train.csv
â”‚   â”‚   â””â”€â”€ dev.csv
â”‚   â”œâ”€â”€ tamil/
â”‚   â”‚   â”œâ”€â”€ train.csv
â”‚   â”‚   â””â”€â”€ dev.csv
â”‚   â””â”€â”€ telugu/
â”‚       â”œâ”€â”€ train.csv
â”‚       â””â”€â”€ dev.csv
â”œâ”€â”€ Models/
â”‚   â”œâ”€â”€ bangla_gec_mt5/
â”‚   â”‚   â””â”€â”€ best_model/
â”‚   â”œâ”€â”€ hindi_gec_mt5/
â”‚   â”‚   â””â”€â”€ best_model/
â”‚   â”œâ”€â”€ malayalam_gec_mt5/
â”‚   â”‚   â””â”€â”€ best_model/
â”‚   â”œâ”€â”€ tamil_gec_model/
â”‚   â”‚   â””â”€â”€ best_model/
â”‚   â””â”€â”€ telugu_gec_mt5/
â”‚       â””â”€â”€ best_model/
â”œâ”€â”€ Bangla/
â”‚   â”œâ”€â”€ train.py
â”‚   â”œâ”€â”€ evaluate.py
â”‚   â””â”€â”€ inference.py
â”œâ”€â”€ Hindi/
â”‚   â”œâ”€â”€ train.py
â”‚   â”œâ”€â”€ evaluate.py
â”‚   â””â”€â”€ inference.py
â”œâ”€â”€ Malayalam/
â”‚   â”œâ”€â”€ train.py
â”‚   â”œâ”€â”€ evaluate.py
â”‚   â””â”€â”€ inference.py
â”œâ”€â”€ Tamil/
â”‚   â”œâ”€â”€ train.py
â”‚   â”œâ”€â”€ evaluate.py
â”‚   â””â”€â”€ inference.py
â”œâ”€â”€ Telugu/
â”‚   â”œâ”€â”€ train.py
â”‚   â”œâ”€â”€ evaluate.py
â”‚   â””â”€â”€ inference.py
â””â”€â”€ README.md
```

## ğŸš€ Quick Start

### Prerequisites

```bash
pip install torch transformers datasets pandas nltk
```

### Training

#### Tamil (91 samples)
```bash
cd Tamil
python train.py
```

**Configuration:**
- Model: `google/mt5-small`
- Batch Size: 2 (effective: 4 with gradient accumulation)
- Final Loss: 1.22

#### Telugu (599 samples)
```bash
cd Telugu
python train.py
```

**Configuration:**
- Model: `google/mt5-small`
- Batch Size: 4 (effective: 4 with gradient accumulation)
- Final Loss: ~2.1-2.4

#### Bangla (598 samples)
```bash
cd Bangla
python train.py
```

**Configuration:**
- Model: `google/mt5-small`
- Batch Size: 4 (effective: 4 with gradient accumulation)
- Final Loss: ~2.0-2.5

#### Hindi (600 samples)
```bash
cd Hindi
python train.py
```

**Configuration:**
- Model: `google/mt5-small`
- Batch Size: 4 (effective: 4 with gradient accumulation)

#### Malayalam (313 samples)
```bash
cd Malayalam
python train.py
```

**Configuration:**
- Model: `google/mt5-small`
- Batch Size: 4 (effective: 4 with gradient accumulation)

### Evaluation

```bash
# Tamil
cd Tamil
python evaluate.py

# Telugu
cd Telugu
python evaluate.py

# Bangla
cd Bangla
python evaluate.py

# Hindi
cd Hindi
python evaluate.py

# Malayalam
cd Malayalam
python evaluate.py
```

Generates `evaluation_results.json` with:
- GLEU Score
- BLEU Score
- Character Error Rate (CER)
- Sample predictions

## ğŸ“Š Results

### Tamil (mT5-small)
| Metric | Score |
|--------|-------|
| **GLEU Score** | **0.5344** |
| **BLEU Score** | 0.5059 |
| **Character Error Rate** | 0.9917 |
| **Training Samples** | 91 |
| **Test Samples** | 16 |

### Telugu (mT5-small)
| Metric | Score |
|--------|-------|
| **GLEU Score** | **0.7217** |
| **BLEU Score** | 0.6902 |
| **Character Error Rate** | 0.2987 |
| **Training Samples** | 539 |
| **Test Samples** | 100 |

### Bangla (mT5-small)
| Metric | Score |
|--------|-------|
| **GLEU Score** | **0.9278** |
| **BLEU Score** | 0.9252 |
| **Character Error Rate** | 0.0442 |
| **Training Samples** | 538 |
| **Test Samples** | 101 |

### Hindi (mT5-small)
| Metric | Score |
|--------|-------|
| **GLEU Score** | **0.8236** |
| **BLEU Score** | 0.8098 |
| **Character Error Rate** | 0.2126 |
| **Training Samples** | 600 |
| **Test Samples** | 107 |

### Malayalam (mT5-small)
| Metric | Score |
|--------|-------|
| **GLEU Score** | **0.6725** |
| **BLEU Score** | 0.6470 |
| **Character Error Rate** | 0.4401 |
| **Training Samples** | 313 |
| **Test Samples** | 50 |

### Key Findings
- **Bangla achieves highest performance:** Bangla (538 samples, GLEU 0.93) shows exceptional results, followed by Hindi (600 samples, GLEU 0.82)
- **Data quality matters:** Well-curated datasets lead to better performance
- **Low-resource effectiveness:** mT5-small performs well even with minimal training data (91 samples for Tamil achieves 0.53 GLEU)
- **Scalable approach:** Works efficiently on consumer-grade hardware (RTX 3050 4GB)

### Inference

```bash
# Tamil
cd Tamil
python inference.py

# Telugu
cd Telugu
python inference.py

# Bangla
cd Bangla
python inference.py

# Hindi
cd Hindi
python inference.py

# Malayalam
cd Malayalam
python inference.py
```

Interactive mode for testing corrections on custom sentences.

## ğŸ“ Examples

### Tamil Language
```
Input:  -à®’à®³à®¿à®ªà¯à®ªà¯†à®°à¯à®•à¯à®•à®¿ à®ªà®¯à®©à¯à®ªà®Ÿà¯à®¤à¯à®¤à¯à®¤à®²à¯
Output: à®‡à®¤à®©à®¾à®²à¯ à®’à®³à®¿à®ªà¯à®ªà¯†à®°à¯à®•à¯à®•à®¿ à®ªà®¯à®©à¯à®ªà®Ÿà¯à®¤à¯à®¤à¯à®¤à®²à¯
Reference: -à®’à®²à®¿à®ªà¯à®ªà¯†à®°à¯à®•à¯à®•à®¿ à®ªà®¯à®©à¯à®ªà®Ÿà¯à®¤à¯à®¤à¯à®¤à®²à¯
GLEU: 0.5344
```

### Telugu Language
```
Input:  à°²à°•à±à°¨à±‹ à°šà±‡à°°à±à°•à±à°¨à±‡à°¸à°°à°¿à°•à°¿ à°¸à°®à°¯à°‚ à°à°¨à°¿à°®à°¿à°¦à°¿ à°…à°¯à±à°¯à°¿à°‚à°¦à°¿.
Output: à°®à°¾à°•à±à°¨à±‹ à°šà±‡à°°à±à°•à±à°¨à±‡à°¸à°°à°¿à°•à°¿ à°¸à°®à°¯à°‚ à°à°¨à°¿à°®à°¿à°¦à°¿ à°…à°¯à±à°¯à°¿à°‚à°¦à°¿.
Reference: à°²à°–à°¨à±Œ à°šà±‡à°°à±à°•à±à°¨à±‡ à°¸à°°à°¿à°•à°¿ à°¸à°®à°¯à°‚ à°à°¨à°¿à°®à°¿à°¦à°¿ à°…à°¯à±à°¯à°¿à°‚à°¦à°¿.
GLEU: 0.7217
```

### Bangla Language
```
Input:  à¦“à¦‡ à¦°à§‚à¦ª à¦à¦¬à¦‚ à¦“à¦‡ à¦°à§à¦šà¦¿à¦° à¦®à§‚à¦²à§à¦¯ à¦•à§€ à¦•à¦°à¦– à¦¦à§‡à¦“à¦¯à¦¼à¦¾ à¦¯à¦¾à¦¯à¦¼ à¦¤à¦¾à¦‡ à¦­à¦¾à¦¬à¦›à¦¿à¥¤
Output: à¦“à¦‡ à¦°à§‚à¦ª à¦à¦¬à¦‚ à¦“à¦‡ à¦°à§à¦šà¦¿à¦° à¦®à§‚à¦²à§à¦¯ à¦•à§€ à¦•à¦°à¦– à¦¦à§‡à¦“à¦¯à¦¼à¦¾ à¦¯à¦¾à¦¯à¦¼ à¦¤à¦¾à¦‡ à¦­à¦¾à¦¬à¦›à¦¿à¥¤
Reference: à¦“à¦‡ à¦°à§‚à¦ª à¦à¦¬à¦‚ à¦“à¦‡ à¦°à§à¦šà¦¿à¦° à¦®à§‚à¦²à§à¦¯ à¦•à§€ à¦•à¦°à§‡ à¦¦à§‡à¦“à¦¯à¦¼à¦¾ à¦¯à¦¾à¦¯à¦¼ à¦¤à¦¾à¦‡ à¦­à¦¾à¦¬à¦›à¦¿à¥¤
GLEU: 0.9278
```

### Hindi Language
```
Input:  à¤¦à¤°à¤…à¤¸à¤² à¤®à¤¾à¤¨à¤µà¥€à¤¯ à¤—à¤¤à¤¿à¤µà¤¿à¤§à¤¿à¤¯à¤¾ à¤œà¥ˆà¤¸à¥‡ à¤•à¤¿ à¤¶à¤¹à¤°à¥€à¤•à¤°à¤£, à¤”à¤¦à¥à¤¯à¥‹à¤—à¤¿à¤•à¤°à¤£ à¤‡à¤¤à¥à¤¯à¤¾à¤¦à¤¿ à¤•à¥‡ à¤•à¤¾à¤°à¤£ à¤µà¤¿à¤¶à¥à¤µ à¤•à¤¾ à¤¤à¤¾à¤ªà¤®à¤¾à¤¨ à¤¤à¥‡à¤œà¥€ à¤¸à¥‡ à¤¬à¤¢à¤¼ à¤°à¤¹à¤¾ à¤¹à¥ˆà¥¤
Output: à¤ªà¤°à¤…à¤¸à¤² à¤®à¤¾à¤¨à¤µà¥€à¤¯ à¤—à¤¤à¤¿à¤µà¤¿à¤§à¤¿à¤¯à¤¾ à¤œà¥ˆà¤¸à¥‡ à¤•à¤¿ à¤¶à¤¹à¤°à¥€à¤•à¤°à¤£, à¤”à¤¦à¥à¤¯à¥‹à¤—à¤¿à¤•à¤°à¤£ à¤‡à¤¤à¥à¤¯à¤¾à¤¦à¤¿ à¤•à¥‡ à¤•à¤¾à¤°à¤£ à¤µà¤¿à¤¶à¥à¤µ à¤•à¤¾ à¤¤à¤¾à¤ªà¤®à¤¾à¤¨ à¤¤à¥‡à¤œà¥€ à¤¸à¥‡ à¤¬à¤¢à¤¼ à¤°à¤¹à¤¾ à¤¹à¥ˆà¥¤
Reference: à¤¦à¤°à¤…à¤¸à¤² à¤®à¤¾à¤¨à¤µà¥€à¤¯ à¤—à¤¤à¤¿à¤µà¤¿à¤§à¤¿à¤¯à¤¾à¤‚ à¤œà¥ˆà¤¸à¥‡ à¤•à¤¿ à¤¶à¤¹à¤°à¥€à¤•à¤°à¤£, à¤”à¤¦à¥à¤¯à¥‹à¤—à¥€à¤•à¤°à¤£ à¤‡à¤¤à¥à¤¯à¤¾à¤¦à¤¿ à¤•à¥‡ à¤•à¤¾à¤°à¤£ à¤µà¤¿à¤¶à¥à¤µ à¤•à¤¾ à¤¤à¤¾à¤ªà¤®à¤¾à¤¨ à¤¤à¥‡à¤œà¥€ à¤¸à¥‡ à¤¬à¤¢à¤¼ à¤°à¤¹à¤¾ à¤¹à¥ˆà¥¤
GLEU: 0.8236
```

### Malayalam Language
```
Input:  à´¨à´®àµà´®à´³àµà´³àµà´Ÿàµ† à´œàµ€à´µà´¶àµˆà´²à´¿à´•àµà´•à´¨àµà´¸à´°à´¿à´šàµà´šàµ à´®à´¾à´²à´¿à´¨àµà´¯à´™àµà´™àµ¾ à´‰à´£àµà´Ÿà´¾à´•àµà´‚ à´à´¨àµà´¨à´¤à´¿àµ½ à´¸à´‚à´¶à´¯à´®à´¿à´²àµà´².
Output: à´†à´²à´¿à´¨àµà´¯à´™àµà´™àµ¾ à´‰à´£àµà´Ÿà´¾à´•àµà´‚ à´à´¨àµà´¨à´¤à´¿àµ½ à´¸à´‚à´¶à´¯à´®à´¿à´²àµà´².
Reference: à´¨à´®àµà´®àµà´Ÿàµ† à´œàµ€à´µà´¿à´¤à´¶àµˆà´²à´¿à´•àµà´•à´¨àµà´¸à´°à´¿à´šàµà´šàµ à´®à´¾à´²à´¿à´¨àµà´¯à´™àµà´™àµ¾ à´‰à´£àµà´Ÿà´¾à´•àµà´‚ à´à´¨àµà´¨à´¤à´¿àµ½ à´¸à´‚à´¶à´¯à´®à´¿à´²àµà´².
GLEU: 0.6725
```

## ğŸ¯ Model Architecture

### mT5-small (Multilingual T5)
- **Parameters:** ~300M
- **Architecture:** Encoder-Decoder Transformer
- **Tokenizer:** SentencePiece (handles all Indic scripts)
- **Max Sequence Length:** 64 tokens (optimized for memory)
- **Languages:** 101 languages including all major Indian languages
- **Pre-training Corpus:** Multilingual C4 (mC4)
- **Pre-training Task:** Span corruption (mask and predict text spans)

### Why mT5 for Low-Resource Indian Languages?

**1. Multilingual Pre-training**
- Trained on massive corpus covering Tamil, Telugu, Hindi, Malayalam, Bengali
- Cross-lingual transfer learning from high-resource to low-resource languages
- Shared vocabulary across Indic scripts

**2. Strong Performance**
- Telugu: GLEU 0.72 with just 599 training samples
- Tamil: GLEU 0.53 with only 91 training samples
- Fast convergence with limited data

**3. Resource Efficiency**
- Works on 4GB VRAM (RTX 3050)
- No need for expensive hardware
- Optimized for low-bandwidth environments

### Training Strategy
- **Data Usage:** All training samples used (no train/val split for small datasets)
- **Gradient Accumulation:** 2-4 steps (effective batch size: 4-8)
- **Loss Function:** Cross-entropy with label smoothing (0.1)
- **Gradient Clipping:** Max norm = 1.0
- **Memory Optimization:** Dynamic padding, FP16 disabled for stability
- **No evaluation during training:** Saves time and memory

## ğŸ”§ Configuration

### Optimizing for Different Scenarios

**Standard Configuration:**
```python
batch_size = 4
learning_rate = 5e-5
gradient_accumulation_steps = 1
```

**Small Datasets:**
```python
batch_size = 2
learning_rate = 1e-4
gradient_accumulation_steps = 2
```

**Low Memory (< 4GB VRAM):**
```python
batch_size = 1
gradient_accumulation_steps = 4
max_length = 32
fp16 = False
# Required for GPUs with limited VRAM
```

**High Memory (> 8GB VRAM):**
```python
batch_size = 8
gradient_accumulation_steps = 1
max_length = 128
fp16 = True
# For faster training with powerful GPUs
```

## ğŸ“Š Evaluation Metrics

### GLEU Score (Primary)
- **Range:** 0.0 to 1.0
- **Interpretation:**
  - \> 0.7: Excellent performance âœ… (Telugu)
  - 0.5 - 0.7: Good performance âœ… (Tamil)
  - 0.3 - 0.5: Acceptable
  - < 0.3: Needs improvement
- **Character-level evaluation** for Indic scripts
- **Sensitive to spacing and punctuation**

### BLEU Score
- Standard metric for sequence-to-sequence tasks
- Character-level evaluation for Indic languages
- Complements GLEU score

### Character Error Rate (CER)
- **Formula:** `(insertions + deletions + substitutions) / total_characters`
- **Lower is better**
- Telugu: 0.30 (Good)
- Tamil: 0.99 (Needs improvement)

## ğŸ› Troubleshooting

### Out of Memory Error
```python
# Reduce batch size in train.py
batch_size = 1
gradient_accumulation_steps = 4
```

### Slow Training
```python
# Increase batch size (if memory allows)
batch_size = 4

# Reduce epochs
epochs = 10

# Disable evaluation during training (already done)
eval_strategy = "no"
```

### Poor Results
- **Check for spacing issues:** GLEU is sensitive to whitespace
- **Collect more training data:** More data = better results (Telugu vs Tamil)
- **Adjust learning rate:** Try 5e-5 to 1e-4
- **Verify data quality:** Check for inconsistent annotations

### Model Download Issues
```python
# Use PyTorch format instead of SafeTensors (smaller, faster)
use_safetensors=False

# Already implemented in code for bandwidth optimization
```

### `<extra_id_0>` tokens in output
- Normal for T5 models during training
- Use `skip_special_tokens=True` in decoding (already done)

## ğŸ“š Dataset Format

CSV files with two columns:

| Input sentence | Output sentence |
|---------------|-----------------|
| Incorrect text | Corrected text |

**Example (`train.csv`):**
```csv
Input sentence,Output sentence
à®‡à®¨à¯à®¤ à®µà®¾à®•à¯à®•à®¿à®¯ à®¤à®µà®±à¯,à®‡à®¨à¯à®¤ à®µà®¾à®•à¯à®•à®¿à®¯à®®à¯ à®¤à®µà®±à¯
```

## ğŸ“„ License

This project is developed for academic purposes.